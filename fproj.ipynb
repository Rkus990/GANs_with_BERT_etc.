{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUpqAwtN8rTA"
      },
      "source": [
        "# GAN-BERT (in Pytorch and compatible with HuggingFace)\n",
        "\n",
        "This is a Pytorch (+ **Huggingface** transformers) implementation of the GAN-BERT model from https://github.com/crux82/ganbert. While the original GAN-BERT was an extension of BERT, this implementation can be adapted to several architectures, ranging from Roberta to Albert!\n",
        "\n",
        "**NOTE**: given that this implementation is different from the original one in Tensorflow, some results can be slighty different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0m5KR34gmRH"
      },
      "source": [
        "Let's GO!\n",
        "\n",
        "Required Imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIqpm34x2rms",
        "outputId": "443d5e17-8483-4155-94af-bc5a2b2d8e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.3.2\n",
            "  Downloading transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 58.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (3.7.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.3.2) (4.64.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.3.2) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.3.2) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.3.2) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.3.2) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=bde240e861ddd6a6cc0dbba8c1f27354963a851d0be4bc855b3e4345a5a25778\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.3.2\n",
        "import torch\n",
        "import io\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "import torch.nn as nn\n",
        "from transformers import *\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "#!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install sentencepiece\n",
        "\n",
        "##Set random values\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZgRup520II",
        "outputId": "35e07929-4dc9-447b-f69e-df08b66aebd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU3ns8Ic7I-h"
      },
      "source": [
        "### Input Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw0HC_hU3FUy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "203c0fff-6370-4e03-f94a-69ab6d8983e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'label_list = [\"UNK_UNK\",\"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \\n              \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \\n              \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \\n              \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \\n              \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \\n              \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \\n              \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \\n              \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \\n              \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \\n              \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \\n              \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \\n              \"NUM_weight\"]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 64\n",
        "batch_size = 64\n",
        "\n",
        "#--------------------------------\n",
        "#  GAN-BERT specific parameters\n",
        "#--------------------------------\n",
        "# number of hidden layers in the generator, \n",
        "# each of the size of the output space\n",
        "num_hidden_layers_g = 1; \n",
        "# number of hidden layers in the discriminator, \n",
        "# each of the size of the input space\n",
        "num_hidden_layers_d = 1; \n",
        "# size of the generator's input noisy vectors\n",
        "noise_size = 100\n",
        "# dropout to be applied to discriminator's input vectors\n",
        "out_dropout_rate = 0.2\n",
        "\n",
        "# Replicate labeled data to balance poorly represented datasets, \n",
        "# e.g., less than 1% of labeled material\n",
        "apply_balance = True\n",
        "\n",
        "#--------------------------------\n",
        "#  Optimization parameters\n",
        "#--------------------------------\n",
        "learning_rate_discriminator = 5e-5\n",
        "learning_rate_generator = 5e-5\n",
        "epsilon = 1e-8\n",
        "num_train_epochs = 5\n",
        "multi_gpu = True\n",
        "# Scheduler\n",
        "apply_scheduler = False\n",
        "warmup_proportion = 0.1\n",
        "# Print\n",
        "print_each_n_step = 10\n",
        "\n",
        "#--------------------------------\n",
        "#  Adopted Tranformer model\n",
        "#--------------------------------\n",
        "# Since this version is compatible with Huggingface transformers, you can uncomment\n",
        "# (or add) transformer models compatible with GAN\n",
        "\n",
        "#model_name = \"bert-base-cased\"\n",
        "#model_name = \"bert-base-uncased\"\n",
        "#model_name = \"roberta-base\"\n",
        "#model_name = \"albert-base-v2\"\n",
        "#model_name = \"xlm-roberta-base\"\n",
        "model_name = \"xlnet-base-cased\"\n",
        "#model_name = \"prajjwal1/bert-tiny\"\n",
        "\n",
        "#--------------------------------\n",
        "#  Retrieve the TREC QC Dataset\n",
        "#--------------------------------\n",
        "#! git clone https://github.com/crux82/ganbert\n",
        "\n",
        "#  NOTE: in this setting 50 classes are involved\n",
        "labeled_file = \"./ganbert/data/labeled.tsv\"\n",
        "unlabeled_file = \"./ganbert/data/unlabeled.tsv\"\n",
        "test_filename = \"./ganbert/data/test.tsv\"\n",
        "\n",
        "\"\"\"label_list = [\"UNK_UNK\",\"ABBR_abb\", \"ABBR_exp\", \"DESC_def\", \"DESC_desc\", \n",
        "              \"DESC_manner\", \"DESC_reason\", \"ENTY_animal\", \"ENTY_body\", \n",
        "              \"ENTY_color\", \"ENTY_cremat\", \"ENTY_currency\", \"ENTY_dismed\", \n",
        "              \"ENTY_event\", \"ENTY_food\", \"ENTY_instru\", \"ENTY_lang\", \n",
        "              \"ENTY_letter\", \"ENTY_other\", \"ENTY_plant\", \"ENTY_product\", \n",
        "              \"ENTY_religion\", \"ENTY_sport\", \"ENTY_substance\", \"ENTY_symbol\", \n",
        "              \"ENTY_techmeth\", \"ENTY_termeq\", \"ENTY_veh\", \"ENTY_word\", \"HUM_desc\", \n",
        "              \"HUM_gr\", \"HUM_ind\", \"HUM_title\", \"LOC_city\", \"LOC_country\", \n",
        "              \"LOC_mount\", \"LOC_other\", \"LOC_state\", \"NUM_code\", \"NUM_count\", \n",
        "              \"NUM_date\", \"NUM_dist\", \"NUM_money\", \"NUM_ord\", \"NUM_other\", \n",
        "              \"NUM_perc\", \"NUM_period\", \"NUM_speed\", \"NUM_temp\", \"NUM_volsize\", \n",
        "              \"NUM_weight\"]\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6Q5jzVioTHb"
      },
      "source": [
        "Load the Tranformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxghkkZq3Gbn"
      },
      "outputs": [],
      "source": [
        "transformer = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd_ixn5qn_zV"
      },
      "source": [
        "Function required to load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7cP8q7K3BId"
      },
      "outputs": [],
      "source": [
        "def get_qc_examples(input_file):\n",
        "  \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "  examples = []\n",
        "\n",
        "  with open(input_file, 'r') as f:\n",
        "      contents = f.read()\n",
        "      file_as_list = contents.splitlines()\n",
        "      for line in file_as_list[1:]:\n",
        "          split = line.split(\" \")\n",
        "          question = ' '.join(split[1:])\n",
        "\n",
        "          text_a = question\n",
        "          inn_split = split[0].split(\":\")\n",
        "          label = inn_split[0] + \"_\" + inn_split[1]\n",
        "          examples.append((text_a, label))\n",
        "      f.close()\n",
        "\n",
        "  return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709,
          "referenced_widgets": [
            "e7414dfbab0c4e6e9ff6f99f43bb980b",
            "164b9566c36d45099f69e175719d31d9",
            "64fd8a7e42de4a499584184461fa25e1",
            "ef7a09b873de42db89504cd7a582e3ca",
            "057a1219b0c1435c90ac6c8828a91d5f",
            "63860f71c861473794b4d252095e8a92",
            "1841d7ef69164b1aaabc82535c2d3094",
            "5cf088a868374aefa5c6894e94678169",
            "6a48400ceabd4a5793d2eac115d63b0b",
            "bbf34eb645fa4bf8b82f05334e84ec43",
            "1d805f8a0cd64683b2ef791c9386c5ec"
          ]
        },
        "id": "_MrWPcPzDVzK",
        "outputId": "d89feebe-e881-4b51-b9c1-289ec9195a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: dill<0.3.5 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset multi_nli (/root/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7414dfbab0c4e6e9ff6f99f43bb980b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"load multinli dataset\n",
        "run this before pip installing the other stuff btw\"\"\"\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"multi_nli\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hapkKJIRcrRg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "px7A5rb3SA6y"
      },
      "outputs": [],
      "source": [
        "\"\"\"multinli dataset formatting\"\"\"\n",
        "#--------------------------------\n",
        "#  Transformer parameters\n",
        "#--------------------------------\n",
        "max_seq_length = 64\n",
        "batch_size = 64\n",
        "num_train_epochs = 5\n",
        "\n",
        "numle = 1500\n",
        "numunle = 3500\n",
        "numtest = 500\n",
        "\n",
        "labeled_examples = [(dataset['train'][i]['premise'],dataset['train'][i]['genre']) for i in range(0,numle)]\n",
        "unlabeled_examples = [(dataset['train'][numle+i]['premise'],'unk') for i in range(0,numunle)]\n",
        "test_examples = [(dataset['train'][numle+numunle+i]['premise'],dataset['train'][numle+numunle+i]['genre']) for i in range(0,numtest)]\n",
        "\n",
        "label_list = list(set(dataset['train'][0:(numle+numunle+numtest)]['genre']))\n",
        "label_list.append('unk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHKDsDSxbGCu"
      },
      "outputs": [],
      "source": [
        "label_list = list(set(dataset['train']['genre']))\n",
        "label_list.append('unk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKU-sbbNVMgP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af63d3ee-65e1-40c9-e7b6-daf1c9abd86f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['telephone', 'slate', 'government', 'travel', 'fiction', 'unk']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "label_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K43tOavNqib4"
      },
      "source": [
        "**Load** the input QC dataset (fine-grained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXCwFyF2qhw7"
      },
      "outputs": [],
      "source": [
        "#Load the examples\n",
        "labeled_examples = get_qc_examples(labeled_file)\n",
        "unlabeled_examples = get_qc_examples(unlabeled_file)\n",
        "test_examples = get_qc_examples(test_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhaW5vBfR6B"
      },
      "source": [
        "Functions required to convert examples into Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmKL5AD7I4Zg"
      },
      "outputs": [],
      "source": [
        "def generate_data_loader(input_examples, label_masks, label_map, do_shuffle = False, balance_label_examples = False):\n",
        "  '''\n",
        "  Generate a Dataloader given the input examples, eventually masked if they are \n",
        "  to be considered NOT labeled.\n",
        "  '''\n",
        "  examples = []\n",
        "\n",
        "  # Count the percentage of labeled examples  \n",
        "  num_labeled_examples = 0\n",
        "  for label_mask in label_masks:\n",
        "    if label_mask: \n",
        "      num_labeled_examples += 1\n",
        "  label_mask_rate = num_labeled_examples/len(input_examples)\n",
        "\n",
        "  # if required it applies the balance\n",
        "  for index, ex in enumerate(input_examples): \n",
        "    if label_mask_rate == 1 or not balance_label_examples:\n",
        "      examples.append((ex, label_masks[index]))\n",
        "    else:\n",
        "      # IT SIMULATE A LABELED EXAMPLE\n",
        "      if label_masks[index]:\n",
        "        balance = int(1/label_mask_rate)\n",
        "        balance = int(math.log(balance,2))\n",
        "        if balance < 1:\n",
        "          balance = 1\n",
        "        for b in range(0, int(balance)):\n",
        "          examples.append((ex, label_masks[index]))\n",
        "      else:\n",
        "        examples.append((ex, label_masks[index]))\n",
        "  \n",
        "  #-----------------------------------------------\n",
        "  # Generate input examples to the Transformer\n",
        "  #-----------------------------------------------\n",
        "  input_ids = []\n",
        "  input_mask_array = []\n",
        "  label_mask_array = []\n",
        "  label_id_array = []\n",
        "\n",
        "  # Tokenization \n",
        "  for (text, label_mask) in examples:\n",
        "    encoded_sent = tokenizer.encode(text[0], add_special_tokens=True, max_length=max_seq_length, padding=\"max_length\", truncation=True)\n",
        "    input_ids.append(encoded_sent)\n",
        "    label_id_array.append(label_map[text[1]])\n",
        "    label_mask_array.append(label_mask)\n",
        "  \n",
        "  # Attention to token (to ignore padded input wordpieces)\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]                          \n",
        "    input_mask_array.append(att_mask)\n",
        "  # Convertion to Tensor\n",
        "  input_ids = torch.tensor(input_ids) \n",
        "  input_mask_array = torch.tensor(input_mask_array)\n",
        "  label_id_array = torch.tensor(label_id_array, dtype=torch.long)\n",
        "  label_mask_array = torch.tensor(label_mask_array)\n",
        "\n",
        "  # Building the TensorDataset\n",
        "  dataset = TensorDataset(input_ids, input_mask_array, label_id_array, label_mask_array)\n",
        "\n",
        "  if do_shuffle:\n",
        "    sampler = RandomSampler\n",
        "  else:\n",
        "    sampler = SequentialSampler\n",
        "\n",
        "  # Building the DataLoader\n",
        "  return DataLoader(\n",
        "              dataset,  # The training samples.\n",
        "              sampler = sampler(dataset), \n",
        "              batch_size = batch_size) # Trains with this batch size.\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do3O-VeefT3g"
      },
      "source": [
        "Convert the input examples into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c-nsMXlKX-D"
      },
      "outputs": [],
      "source": [
        "label_map = {}\n",
        "for (i, label) in enumerate(label_list):\n",
        "  label_map[label] = i\n",
        "#------------------------------\n",
        "#   Load the train dataset\n",
        "#------------------------------\n",
        "train_examples = labeled_examples\n",
        "#The labeled (train) dataset is assigned with a mask set to True\n",
        "train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "#If unlabel examples are available\n",
        "if unlabeled_examples:\n",
        "  train_examples = train_examples + unlabeled_examples\n",
        "  #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "  tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "  train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
        "\n",
        "#------------------------------\n",
        "#   Load the test dataset\n",
        "#------------------------------\n",
        "#The labeled (test) dataset is assigned with a mask set to True\n",
        "test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "\n",
        "test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ihcw3vquaQm"
      },
      "source": [
        "We define the Generator and Discriminator as discussed in https://www.aclweb.org/anthology/2020.acl-main.191/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18kY64-n3I6y"
      },
      "outputs": [],
      "source": [
        "#------------------------------\n",
        "#   The Generator as in \n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_size=100, output_size=512, hidden_sizes=[512], dropout_rate=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "        layers = []\n",
        "        hidden_sizes = [noise_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1],output_size))\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, noise):\n",
        "        output_rep = self.layers(noise)\n",
        "        return output_rep\n",
        "\n",
        "#------------------------------\n",
        "#   The Discriminator\n",
        "#   https://www.aclweb.org/anthology/2020.acl-main.191/\n",
        "#   https://github.com/crux82/ganbert\n",
        "#------------------------------\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=512, hidden_sizes=[512], num_labels=2, dropout_rate=0.1):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dropout = nn.Dropout(p=dropout_rate)\n",
        "        layers = []\n",
        "        hidden_sizes = [input_size] + hidden_sizes\n",
        "        for i in range(len(hidden_sizes)-1):\n",
        "            layers.extend([nn.Linear(hidden_sizes[i], hidden_sizes[i+1]), nn.LeakyReLU(0.2, inplace=True), nn.Dropout(dropout_rate)])\n",
        "\n",
        "        self.layers = nn.Sequential(*layers) #per il flatten\n",
        "        self.logit = nn.Linear(hidden_sizes[-1],num_labels+1) # +1 for the probability of this sample being fake/real.\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_rep):\n",
        "        input_rep = self.input_dropout(input_rep)\n",
        "        last_rep = self.layers(input_rep)\n",
        "        logits = self.logit(last_rep)\n",
        "        probs = self.softmax(logits)\n",
        "        return last_rep, logits, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uje9s2zQunFc"
      },
      "source": [
        "We instantiate the Discriminator and Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylz5rvqE3U2S"
      },
      "outputs": [],
      "source": [
        "# The config file is required to get the dimension of the vector produced by \n",
        "# the underlying transformer\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "hidden_size = int(config.hidden_size)\n",
        "# Define the number and width of hidden layers\n",
        "hidden_levels_g = [hidden_size for i in range(0, num_hidden_layers_g)]\n",
        "hidden_levels_d = [hidden_size for i in range(0, num_hidden_layers_d)]\n",
        "\n",
        "#-------------------------------------------------\n",
        "#   Instantiate the Generator and Discriminator\n",
        "#-------------------------------------------------\n",
        "generator = Generator(noise_size=noise_size, output_size=hidden_size, hidden_sizes=hidden_levels_g, dropout_rate=out_dropout_rate)\n",
        "discriminator = Discriminator(input_size=hidden_size, hidden_sizes=hidden_levels_d, num_labels=len(label_list), dropout_rate=out_dropout_rate)\n",
        "\n",
        "# Put everything in the GPU if available\n",
        "if torch.cuda.is_available():    \n",
        "  generator.cuda()\n",
        "  discriminator.cuda()\n",
        "  transformer.cuda()\n",
        "  if multi_gpu:\n",
        "    transformer = torch.nn.DataParallel(transformer)\n",
        "\n",
        "# print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG3qzp2-usZE"
      },
      "source": [
        "Let's go with the training procedure"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 64\n",
        "batch_size = 64\n",
        "num_train_epochs = 5\n",
        "tot_num_tr = 5000\n",
        "numtest = 500\n",
        "\n",
        "for percent in [.01,.02,.05,.1,.2,.3,.4,.5]:\n",
        "  print(\"==={:} labeled\".format(percent))\n",
        "  numle = (int) (tot_num_tr * percent)\n",
        "  numunle = (int) (tot_num_tr * (1 - percent))\n",
        "\n",
        "  labeled_examples = [(dataset['train'][i]['premise'],dataset['train'][i]['genre']) for i in range(0,numle)]\n",
        "  unlabeled_examples = [(dataset['train'][numle+i]['premise'],'unk') for i in range(0,numunle)]\n",
        "  test_examples = [(dataset['train'][numle+numunle+i]['premise'],dataset['train'][numle+numunle+i]['genre']) for i in range(0,numtest)]\n",
        "\n",
        "  label_list = list(set(dataset['train'][0:(numle+numunle+numtest)]['genre']))\n",
        "  label_list.append('unk')\n",
        "\n",
        "\n",
        "\n",
        "  label_map = {}\n",
        "  for (i, label) in enumerate(label_list):\n",
        "    label_map[label] = i\n",
        "  #------------------------------\n",
        "  #   Load the train dataset\n",
        "  #------------------------------\n",
        "  train_examples = labeled_examples\n",
        "  #The labeled (train) dataset is assigned with a mask set to True\n",
        "  train_label_masks = np.ones(len(labeled_examples), dtype=bool)\n",
        "  #If unlabel examples are available\n",
        "  if unlabeled_examples:\n",
        "    train_examples = train_examples + unlabeled_examples\n",
        "    #The unlabeled (train) dataset is assigned with a mask set to False\n",
        "    tmp_masks = np.zeros(len(unlabeled_examples), dtype=bool)\n",
        "    train_label_masks = np.concatenate([train_label_masks,tmp_masks])\n",
        "\n",
        "  train_dataloader = generate_data_loader(train_examples, train_label_masks, label_map, do_shuffle = True, balance_label_examples = apply_balance)\n",
        "\n",
        "  #------------------------------\n",
        "  #   Load the test dataset\n",
        "  #------------------------------\n",
        "  #The labeled (test) dataset is assigned with a mask set to True\n",
        "  test_label_masks = np.ones(len(test_examples), dtype=bool)\n",
        "\n",
        "  test_dataloader = generate_data_loader(test_examples, test_label_masks, label_map, do_shuffle = False, balance_label_examples = False)\n",
        "\n",
        "\n",
        "  training_stats = []\n",
        "\n",
        "  # Measure the total training time for the whole run.\n",
        "  total_t0 = time.time()\n",
        "  print(len(train_examples))\n",
        "  #models parameters\n",
        "  transformer_vars = [i for i in transformer.parameters()]\n",
        "  d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "  g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "  #optimizer\n",
        "  dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "  gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
        "\n",
        "  #scheduler\n",
        "  if apply_scheduler:\n",
        "    num_train_examples = len(train_examples)\n",
        "    num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "    scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "                                            num_warmup_steps = num_warmup_steps)\n",
        "    scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "                                            num_warmup_steps = num_warmup_steps)\n",
        "\n",
        "  # For each epoch...\n",
        "  for epoch_i in range(0, num_train_epochs):\n",
        "      # ========================================\n",
        "      #               Training\n",
        "      # ========================================\n",
        "      # Perform one full pass over the training set.\n",
        "      print(\"\")\n",
        "      print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "      print('Training...')\n",
        "\n",
        "      # Measure how long the training epoch takes.\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Reset the total loss for this epoch.\n",
        "      tr_g_loss = 0\n",
        "      tr_d_loss = 0\n",
        "\n",
        "      # Put the model into training mode.\n",
        "      transformer.train() \n",
        "      generator.train()\n",
        "      discriminator.train()\n",
        "\n",
        "      # For each batch of training data...\n",
        "      for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "          # Progress update every print_each_n_step batches.\n",
        "          if step % print_each_n_step == 0 and not step == 0:\n",
        "              # Calculate elapsed time in minutes.\n",
        "              elapsed = format_time(time.time() - t0)\n",
        "              \n",
        "              # Report progress.\n",
        "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "          # Unpack this training batch from our dataloader. \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "          b_label_mask = batch[3].to(device)\n",
        "\n",
        "          real_batch_size = b_input_ids.shape[0]\n",
        "      \n",
        "          # Encode real data in the Transformer\n",
        "          model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "          hidden_states = model_outputs[-1]\n",
        "          \n",
        "          # Generate fake data that should have the same distribution of the ones\n",
        "          # encoded by the transformer. \n",
        "          # First noisy input are used in input to the Generator\n",
        "          noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "          # Gnerate Fake data\n",
        "          gen_rep = generator(noise)\n",
        "\n",
        "          # Generate the output of the Discriminator for real and fake data.\n",
        "          # First, we put together the output of the tranformer and the generator\n",
        "          disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "          # Then, we select the output of the disciminator\n",
        "          features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "          # Finally, we separate the discriminator's output for the real and fake\n",
        "          # data\n",
        "          features_list = torch.split(features, real_batch_size)\n",
        "          D_real_features = features_list[0]\n",
        "          D_fake_features = features_list[1]\n",
        "        \n",
        "          logits_list = torch.split(logits, real_batch_size)\n",
        "          D_real_logits = logits_list[0]\n",
        "          D_fake_logits = logits_list[1]\n",
        "          \n",
        "          probs_list = torch.split(probs, real_batch_size)\n",
        "          D_real_probs = probs_list[0]\n",
        "          D_fake_probs = probs_list[1]\n",
        "\n",
        "          #---------------------------------\n",
        "          #  LOSS evaluation\n",
        "          #---------------------------------\n",
        "          # Generator's LOSS estimation\n",
        "          g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
        "          g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "          g_loss = g_loss_d + g_feat_reg\n",
        "    \n",
        "          # Disciminator's LOSS estimation\n",
        "          logits = D_real_logits[:,0:-1]\n",
        "          log_probs = F.log_softmax(logits, dim=-1)\n",
        "          # The discriminator provides an output for labeled and unlabeled real data\n",
        "          # so the loss evaluated for unlabeled data is ignored (masked)\n",
        "          label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
        "          per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "          per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "          labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "          # It may be the case that a batch does not contain labeled examples, \n",
        "          # so the \"supervised loss\" in this case is not evaluated\n",
        "          if labeled_example_count == 0:\n",
        "            D_L_Supervised = 0\n",
        "          else:\n",
        "            D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "                  \n",
        "          D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "          D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "          d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "          #---------------------------------\n",
        "          #  OPTIMIZATION\n",
        "          #---------------------------------\n",
        "          # Avoid gradient accumulation\n",
        "          gen_optimizer.zero_grad()\n",
        "          dis_optimizer.zero_grad()\n",
        "\n",
        "          # Calculate weigth updates\n",
        "          # retain_graph=True is required since the underlying graph will be deleted after backward\n",
        "          g_loss.backward(retain_graph=True)\n",
        "          d_loss.backward() \n",
        "          \n",
        "          # Apply modifications\n",
        "          gen_optimizer.step()\n",
        "          dis_optimizer.step()\n",
        "\n",
        "          # A detail log of the individual losses\n",
        "          #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
        "          #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
        "          #             g_loss_d, g_feat_reg))\n",
        "\n",
        "          # Save the losses to print them later\n",
        "          tr_g_loss += g_loss.item()\n",
        "          tr_d_loss += d_loss.item()\n",
        "\n",
        "          # Update the learning rate with the scheduler\n",
        "          if apply_scheduler:\n",
        "            scheduler_d.step()\n",
        "            scheduler_g.step()\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "      avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
        "      \n",
        "      # Measure how long this epoch took.\n",
        "      training_time = format_time(time.time() - t0)\n",
        "\n",
        "      print(\"\")\n",
        "      print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
        "      print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "      print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "          \n",
        "      # ========================================\n",
        "      #     TEST ON THE EVALUATION DATASET\n",
        "      # ========================================\n",
        "      # After the completion of each training epoch, measure our performance on\n",
        "      # our test set.\n",
        "      print(\"\")\n",
        "      print(\"Running Test...\")\n",
        "\n",
        "      t0 = time.time()\n",
        "\n",
        "      # Put the model in evaluation mode--the dropout layers behave differently\n",
        "      # during evaluation.\n",
        "      transformer.eval() #maybe redundant\n",
        "      discriminator.eval()\n",
        "      generator.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      total_test_accuracy = 0\n",
        "    \n",
        "      total_test_loss = 0\n",
        "      nb_test_steps = 0\n",
        "\n",
        "      all_preds = []\n",
        "      all_labels_ids = []\n",
        "\n",
        "      #loss\n",
        "      nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "      # Evaluate data for one epoch\n",
        "      for batch in test_dataloader:\n",
        "          \n",
        "          # Unpack this training batch from our dataloader. \n",
        "          b_input_ids = batch[0].to(device)\n",
        "          b_input_mask = batch[1].to(device)\n",
        "          b_labels = batch[2].to(device)\n",
        "          \n",
        "          # Tell pytorch not to bother with constructing the compute graph during\n",
        "          # the forward pass, since this is only needed for backprop (training).\n",
        "          with torch.no_grad():        \n",
        "              model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "              hidden_states = model_outputs[-1]\n",
        "              _, logits, probs = discriminator(hidden_states)\n",
        "              ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "              filtered_logits = logits[:,0:-1]\n",
        "              # Accumulate the test loss.\n",
        "              total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "              \n",
        "          # Accumulate the predictions and the input labels\n",
        "          _, preds = torch.max(filtered_logits, 1)\n",
        "          all_preds += preds.detach().cpu()\n",
        "          all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "      # Report the final accuracy for this validation run.\n",
        "      all_preds = torch.stack(all_preds).numpy()\n",
        "      all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "      test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "      print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "      # Calculate the average loss over all of the batches.\n",
        "      avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "      avg_test_loss = avg_test_loss.item()\n",
        "      \n",
        "      # Measure how long the validation run took.\n",
        "      test_time = format_time(time.time() - t0)\n",
        "      \n",
        "      print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "      print(\"  Test took: {:}\".format(test_time))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "8F6l3Er0FISL",
        "outputId": "cfbd6ba0-3761-40ab-9fc3-fe4c2c57003d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===0.01 labeled\n",
            "5000\n",
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-b4288bd6a7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m           \u001b[0;31m# Generate the output of the Discriminator for real and fake data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m           \u001b[0;31m# First, we put together the output of the tranformer and the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m           \u001b[0mdisciminator_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_rep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m           \u001b[0;31m# Then, we select the output of the disciminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m           \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisciminator_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "x = np.array([1,2,5,10,20,30,40,50])\n",
        "#y = np.array([.358,.330,.410,.624,.770,.65,.838,.822]) #XLM\n",
        "#y = np.array([.372,.380,.476,.552,.626,.682,.714,.752]) #bort\n",
        "\n",
        "\n",
        "#y = 100*np.array([.682,.688,.802,.782,.838,.848,.818,.856]) #albert\n",
        "#y = 100*np.array([.396,.380,.200,.346,.200,.412,.498,.668]) #roberta\n",
        "#y = 100*np.array([.764,.802,.834,.860,.876,.896,.894,.902]) #bert_cased\n",
        "y = 100*np.array([.715,.712,.711,.810,.846,.856,.860,.864]) #tinybert\n",
        "\n",
        "\n",
        "#y = 100 * np.array([[.764,.802,.834,.860,.876,.896,.894,.902], [.396,.380,.200,.346,.200,.412,.498,.668], [.682,.688,.802,.782,.838,.848,.818,.856],[.715,.712,.711,.810,.846,.856,.860,.864]])\n",
        "\n",
        "#fig, ax = plt.subplots()\n",
        "#ax.plot(x.astype('str'), np.transpose(y),**{'marker': 'o'})\n",
        "#ax.set_title('Classification on MultiNLI')\n",
        "#ax.set_xlabel('%annotated examples')\n",
        "#ax.set_ylabel('Accuracy (%)')\n",
        "#ax.set_ybound(lower=0,upper=100)\n",
        "#ax.legend(['GAN-BERT', 'GAN-RoBERTa','GAN-ALBERT','GAN-TinyBERT'],loc='lower right')\n",
        "#plt.show()\n",
        "#fig.savefig('mnli_gan.png')\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x.astype('str'), np.transpose(y),**{'marker': 'o'})\n",
        "ax.set_title('Classification on MultiNLI')\n",
        "ax.set_xlabel('%annotated examples')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_ybound(lower=0,upper=100)\n",
        "ax.legend(['GAN-TinyBERT'],loc='lower right')\n",
        "plt.show()\n",
        "fig.savefig('mnli_tinybert.png')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "a9kIDzs6HJKJ",
        "outputId": "8e7be400-b79a-435d-ce8c-3d787dcbc6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDm9NProRB4c"
      },
      "outputs": [],
      "source": [
        "for stat in training_stats:\n",
        "  print(stat)\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhqylHGK3Va4"
      },
      "outputs": [],
      "source": [
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "print(len(train_examples))\n",
        "#models parameters\n",
        "transformer_vars = [i for i in transformer.parameters()]\n",
        "d_vars = transformer_vars + [v for v in discriminator.parameters()]\n",
        "g_vars = [v for v in generator.parameters()]\n",
        "\n",
        "#optimizer\n",
        "dis_optimizer = torch.optim.AdamW(d_vars, lr=learning_rate_discriminator)\n",
        "gen_optimizer = torch.optim.AdamW(g_vars, lr=learning_rate_generator) \n",
        "\n",
        "#scheduler\n",
        "if apply_scheduler:\n",
        "  num_train_examples = len(train_examples)\n",
        "  num_train_steps = int(num_train_examples / batch_size * num_train_epochs)\n",
        "  num_warmup_steps = int(num_train_steps * warmup_proportion)\n",
        "\n",
        "  scheduler_d = get_constant_schedule_with_warmup(dis_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "  scheduler_g = get_constant_schedule_with_warmup(gen_optimizer, \n",
        "                                           num_warmup_steps = num_warmup_steps)\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, num_train_epochs):\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_train_epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    tr_g_loss = 0\n",
        "    tr_d_loss = 0\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    transformer.train() \n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every print_each_n_step batches.\n",
        "        if step % print_each_n_step == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        b_label_mask = batch[3].to(device)\n",
        "\n",
        "        real_batch_size = b_input_ids.shape[0]\n",
        "     \n",
        "        # Encode real data in the Transformer\n",
        "        model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "        hidden_states = model_outputs[-1]\n",
        "        \n",
        "        # Generate fake data that should have the same distribution of the ones\n",
        "        # encoded by the transformer. \n",
        "        # First noisy input are used in input to the Generator\n",
        "        noise = torch.zeros(real_batch_size, noise_size, device=device).uniform_(0, 1)\n",
        "        # Gnerate Fake data\n",
        "        gen_rep = generator(noise)\n",
        "\n",
        "        # Generate the output of the Discriminator for real and fake data.\n",
        "        # First, we put together the output of the tranformer and the generator\n",
        "        disciminator_input = torch.cat([hidden_states, gen_rep], dim=0)\n",
        "        # Then, we select the output of the disciminator\n",
        "        features, logits, probs = discriminator(disciminator_input)\n",
        "\n",
        "        # Finally, we separate the discriminator's output for the real and fake\n",
        "        # data\n",
        "        features_list = torch.split(features, real_batch_size)\n",
        "        D_real_features = features_list[0]\n",
        "        D_fake_features = features_list[1]\n",
        "      \n",
        "        logits_list = torch.split(logits, real_batch_size)\n",
        "        D_real_logits = logits_list[0]\n",
        "        D_fake_logits = logits_list[1]\n",
        "        \n",
        "        probs_list = torch.split(probs, real_batch_size)\n",
        "        D_real_probs = probs_list[0]\n",
        "        D_fake_probs = probs_list[1]\n",
        "\n",
        "        #---------------------------------\n",
        "        #  LOSS evaluation\n",
        "        #---------------------------------\n",
        "        # Generator's LOSS estimation\n",
        "        g_loss_d = -1 * torch.mean(torch.log(1 - D_fake_probs[:,-1] + epsilon))\n",
        "        g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim=0) - torch.mean(D_fake_features, dim=0), 2))\n",
        "        g_loss = g_loss_d + g_feat_reg\n",
        "  \n",
        "        # Disciminator's LOSS estimation\n",
        "        logits = D_real_logits[:,0:-1]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        # The discriminator provides an output for labeled and unlabeled real data\n",
        "        # so the loss evaluated for unlabeled data is ignored (masked)\n",
        "        label2one_hot = torch.nn.functional.one_hot(b_labels, len(label_list))\n",
        "        per_example_loss = -torch.sum(label2one_hot * log_probs, dim=-1)\n",
        "        per_example_loss = torch.masked_select(per_example_loss, b_label_mask.to(device))\n",
        "        labeled_example_count = per_example_loss.type(torch.float32).numel()\n",
        "\n",
        "        # It may be the case that a batch does not contain labeled examples, \n",
        "        # so the \"supervised loss\" in this case is not evaluated\n",
        "        if labeled_example_count == 0:\n",
        "          D_L_Supervised = 0\n",
        "        else:\n",
        "          D_L_Supervised = torch.div(torch.sum(per_example_loss.to(device)), labeled_example_count)\n",
        "                 \n",
        "        D_L_unsupervised1U = -1 * torch.mean(torch.log(1 - D_real_probs[:, -1] + epsilon))\n",
        "        D_L_unsupervised2U = -1 * torch.mean(torch.log(D_fake_probs[:, -1] + epsilon))\n",
        "        d_loss = D_L_Supervised + D_L_unsupervised1U + D_L_unsupervised2U\n",
        "\n",
        "        #---------------------------------\n",
        "        #  OPTIMIZATION\n",
        "        #---------------------------------\n",
        "        # Avoid gradient accumulation\n",
        "        gen_optimizer.zero_grad()\n",
        "        dis_optimizer.zero_grad()\n",
        "\n",
        "        # Calculate weigth updates\n",
        "        # retain_graph=True is required since the underlying graph will be deleted after backward\n",
        "        g_loss.backward(retain_graph=True)\n",
        "        d_loss.backward() \n",
        "        \n",
        "        # Apply modifications\n",
        "        gen_optimizer.step()\n",
        "        dis_optimizer.step()\n",
        "\n",
        "        # A detail log of the individual losses\n",
        "        #print(\"{0:.4f}\\t{1:.4f}\\t{2:.4f}\\t{3:.4f}\\t{4:.4f}\".\n",
        "        #      format(D_L_Supervised, D_L_unsupervised1U, D_L_unsupervised2U,\n",
        "        #             g_loss_d, g_feat_reg))\n",
        "\n",
        "        # Save the losses to print them later\n",
        "        tr_g_loss += g_loss.item()\n",
        "        tr_d_loss += d_loss.item()\n",
        "\n",
        "        # Update the learning rate with the scheduler\n",
        "        if apply_scheduler:\n",
        "          scheduler_d.step()\n",
        "          scheduler_g.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss_g = tr_g_loss / len(train_dataloader)\n",
        "    avg_train_loss_d = tr_d_loss / len(train_dataloader)             \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss generetor: {0:.3f}\".format(avg_train_loss_g))\n",
        "    print(\"  Average training loss discriminator: {0:.3f}\".format(avg_train_loss_d))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #     TEST ON THE EVALUATION DATASET\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our test set.\n",
        "    print(\"\")\n",
        "    print(\"Running Test...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    transformer.eval() #maybe redundant\n",
        "    discriminator.eval()\n",
        "    generator.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_test_accuracy = 0\n",
        "   \n",
        "    total_test_loss = 0\n",
        "    nb_test_steps = 0\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels_ids = []\n",
        "\n",
        "    #loss\n",
        "    nll_loss = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in test_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "            model_outputs = transformer(b_input_ids, attention_mask=b_input_mask)\n",
        "            hidden_states = model_outputs[-1]\n",
        "            _, logits, probs = discriminator(hidden_states)\n",
        "            ###log_probs = F.log_softmax(probs[:,1:], dim=-1)\n",
        "            filtered_logits = logits[:,0:-1]\n",
        "            # Accumulate the test loss.\n",
        "            total_test_loss += nll_loss(filtered_logits, b_labels)\n",
        "            \n",
        "        # Accumulate the predictions and the input labels\n",
        "        _, preds = torch.max(filtered_logits, 1)\n",
        "        all_preds += preds.detach().cpu()\n",
        "        all_labels_ids += b_labels.detach().cpu()\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    all_preds = torch.stack(all_preds).numpy()\n",
        "    all_labels_ids = torch.stack(all_labels_ids).numpy()\n",
        "    test_accuracy = np.sum(all_preds == all_labels_ids) / len(all_preds)\n",
        "    print(\"  Accuracy: {0:.3f}\".format(test_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    avg_test_loss = avg_test_loss.item()\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    test_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Test Loss: {0:.3f}\".format(avg_test_loss))\n",
        "    print(\"  Test took: {:}\".format(test_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss generator': avg_train_loss_g,\n",
        "            'Training Loss discriminator': avg_train_loss_d,\n",
        "            'Valid. Loss': avg_test_loss,\n",
        "            'Valid. Accur.': test_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Test Time': test_time\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RjAs5l7GQaKt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "fproj.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7414dfbab0c4e6e9ff6f99f43bb980b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_164b9566c36d45099f69e175719d31d9",
              "IPY_MODEL_64fd8a7e42de4a499584184461fa25e1",
              "IPY_MODEL_ef7a09b873de42db89504cd7a582e3ca"
            ],
            "layout": "IPY_MODEL_057a1219b0c1435c90ac6c8828a91d5f"
          }
        },
        "164b9566c36d45099f69e175719d31d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63860f71c861473794b4d252095e8a92",
            "placeholder": "​",
            "style": "IPY_MODEL_1841d7ef69164b1aaabc82535c2d3094",
            "value": "100%"
          }
        },
        "64fd8a7e42de4a499584184461fa25e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cf088a868374aefa5c6894e94678169",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a48400ceabd4a5793d2eac115d63b0b",
            "value": 3
          }
        },
        "ef7a09b873de42db89504cd7a582e3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbf34eb645fa4bf8b82f05334e84ec43",
            "placeholder": "​",
            "style": "IPY_MODEL_1d805f8a0cd64683b2ef791c9386c5ec",
            "value": " 3/3 [00:00&lt;00:00, 59.39it/s]"
          }
        },
        "057a1219b0c1435c90ac6c8828a91d5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63860f71c861473794b4d252095e8a92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1841d7ef69164b1aaabc82535c2d3094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cf088a868374aefa5c6894e94678169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a48400ceabd4a5793d2eac115d63b0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbf34eb645fa4bf8b82f05334e84ec43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d805f8a0cd64683b2ef791c9386c5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}